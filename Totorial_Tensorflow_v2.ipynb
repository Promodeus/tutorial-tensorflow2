{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Totorial Tensorflow v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM77HlPxADqFv5tXo0bSqMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stgolarrain/tutorial-tensorflow2/blob/master/Totorial_Tensorflow_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9duaJAERSYKs",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial RecSys - Tensorflow\n",
        "\n",
        "En este tutorial mostraremos cómo utilizar la librería Tensorflow para la implementación de un modelo de Factorización Matricial.\n",
        "\n",
        "El modelo que implementaremos minimiza la siguiente función objetivo:\n",
        "\n",
        "$\\min_{p, q} \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{D}} (r_{ui} - p_u q_i)^2 + \\lambda (||p_u||^2 + ||q_i||^2)$\n",
        "\n",
        "Donde:\n",
        "- $r_{ui}$ es el rating que le ha asignado el usuario $u$ al ítem $i$ en el set de entrenamiento.\n",
        "- $p_u$ representa el vector latente del usuario.\n",
        "- $q_i$ representa el vector latente del ítem.\n",
        "- $\\lambda$ es la variable regularizadora.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MnKuTBHUJuK",
        "colab_type": "text"
      },
      "source": [
        "## Configuración del entorno\n",
        "\n",
        "En esta sección descargaremos el set de datos que utilizaremos para trabajar e instalaremos las dependencias necesarias para implementar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GScnjhZ1SV5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creamos una carpeta donde guardaremos los datos\n",
        "!mkdir data\n",
        "\n",
        "# Descargamos y descomprimimos los datos\n",
        "!curl -L -o \"data/ml-latest-small.zip\" \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
        "!unzip data/ml-latest-small.zip -d data\n",
        "\n",
        "# Vemos las primeras líneas del archivo para asegurar que la descarga haya sido correcta\n",
        "!head data/ml-latest-small/ratings.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84PjcR8X1Hu",
        "colab_type": "text"
      },
      "source": [
        "Cada línea representa un usuario, ítem del producto, rating y timestamp de la hora que se hizo la evaluación del ítem. Por ejemplo:\n",
        "```\n",
        "1;1193;5;978300760\n",
        "```\n",
        "Significa que el usuario 1 le dió rating 5 al ítem 1193 en el tiempo 978300760. En este [link](https://https://www.timestampconvert.com/?go2=true&offset=4&timestamp=978300760&Submit=++++++Convert+to+Date++++++) podemos ver la transformación de 978300760 a una fecha normal que sería: 31/12/2000 19:12:40 de Chile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbT44Bq5Vef8",
        "colab_type": "text"
      },
      "source": [
        "Instalamos las dependencias necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z4IbAHyVhKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVcrUuzCXelh",
        "colab_type": "text"
      },
      "source": [
        "## Carga y preprocesamiento de datos\n",
        "\n",
        "Los datos a utilizar provienen de un dataset bastante popular ([MovieLens](https://grouplens.org/datasets/movielens/)) y que ha sido utilizado en otros prácticos de este curso. A continuación, se importarán los datos utilizando pandas para facilitar su manejo y obtener algunos valores interesantes de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_JhI4VKXnR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pandas facilita el manejo de datos como tablas\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar datos\n",
        "data_path = 'data/ml-latest-small/ratings.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Imprimir tamaños relevantes\n",
        "print(\"Numero de usuarios: {}\".format(data['userId'].nunique()))\n",
        "print(\"Numero de items: {}\".format(data['movieId'].nunique()))\n",
        "print(\"Numero de datos (filas): {}\".format(data.shape[0]))\n",
        "print()\n",
        "\n",
        "# Vemos las primero 5 filas de los datos\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw5SfN53ZtMf",
        "colab_type": "text"
      },
      "source": [
        "Ahora utilizaremos diccionarios para transformar los identificadores de usuario y películas a nuevos identificadores, que van desde 0 hasta N (la cantidad de datos de cada entidad):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU1PHsctY5hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A cada ID del dataset, le asigno un nuevo ID de 0 a N\n",
        "user_to_int = {user: i for i, user in enumerate(data['userId'].unique())}\n",
        "item_to_int = {item: i for i, item in enumerate(data['movieId'].unique())}\n",
        "\n",
        "# Aplicamos la transformacion de ID ya creada\n",
        "data['userId'] = data['userId'].map(lambda user: user_to_int[user])\n",
        "data['movieId'] = data['movieId'].map(lambda item: item_to_int[item])\n",
        "\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbCEx_iWaI7Z",
        "colab_type": "text"
      },
      "source": [
        "El total de los datos los separaremos en un set de entrenamiento (80%) y en un set de validación (20%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh8aDSuwY7LA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "print(f'Datos de entrenamiento: {train_data.shape[0]}')\n",
        "print(f'Datos de validación: {val_data.shape[0]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onwe9qFAa_0h",
        "colab_type": "text"
      },
      "source": [
        "Por último ambos set de datos los transformaremos en un objeto [DataSet](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), lo que genera un _stream_ de datos configurable para conseguir los datos en forma de _batch_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPZ1JfWzajSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(df, batch_size=128):\n",
        "  x = df[['userId', 'movieId']].values\n",
        "  y = df[['rating']].values\n",
        "  \n",
        "  ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n",
        "\n",
        "train_ds = create_dataset(train_data)\n",
        "val_ds = create_dataset(val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sOKvaqKdNFO",
        "colab_type": "text"
      },
      "source": [
        "El objeto DataSet puede ser usado como un iterador donde cada iteración devuelve los datos de entrenamiento en formato _batch_: Los input $x$ y outptu $y$. En este caso los datos $x$ son pares (`userId`, `movieId`) y los datos $y$ corresponden a los datos de `rating`. Cada _batch_ es de largo 32 para este ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFoaQk38cXeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ejemplo de cómo iterar sobre el dataset\n",
        "for (x, y) in train_ds:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63xGWe2gDyo",
        "colab_type": "text"
      },
      "source": [
        "## Factorización matricial\n",
        "\n",
        "En esta sección implementaremos el modelo con la API de tensorflow y luego entrenaremos el modelo con los datos descargados.\n",
        "\n",
        "La implementación del modelo se compone en dos partes:\n",
        "1.   En la primera parte implementaremos la capa interna, en la cual implementaremos una clase `Layer` de la API de Keras. Se recomienda ver la [documentación](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_layer_class) de dicha clase. Las clases `Layer` encapsulan los \"pesos\" de los modelos y su cómputo interno. Para efectos de la factorización matricial, los pesos son las matrices $P$ y $Q$.\n",
        "2.   En la segunda parte implementaremos una clase `Model` de la API de Keras. Al igual que la clase `Layer` se recomienda revisar la [documentación](https://www.tensorflow.org/guide/keras/custom_layers_and_models#building_models) para más detalles. Las clases `Model` encapsulan la capa externa del modelo que queremos implementar y definen métodos ya implementados que son de alta utilida (`model.fit()`, `model.evaluate()`, `model.predict()`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gU0LCrZwXea",
        "colab_type": "text"
      },
      "source": [
        "Ahora continuaremos implementando la capa interna del modelo. La capa interna crea las variables ($P$ y $Q$) que serán modificadas en el proceso de entrenamiento. La clase implementa 3 métodos principales:\n",
        "\n",
        "*   `__init__`: inicializa la clase y asigna internamente los parámetros del modelo, i.e. los tamaños de las matrices $P$ y $Q$.\n",
        "*   `build`: construye las variables necesarias para realizar los cálculos internos.\n",
        "*   `call`: recibe como parámetros los índice de los usuarios e items, busca los _embeddings_ y realiza la multiplicación de los mismos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOoCBSy5gOqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementación de la clase `Layer`\n",
        "class FactorizationLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_users, n_items, k=32):\n",
        "    super().__init__()\n",
        "    self.k = k\n",
        "    self.n_users = n_users\n",
        "    self.n_items = n_items\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # Creamos la matriz P con los vectores latentes de los usuarios\n",
        "    self.P = self.add_variable('P_matrix',\n",
        "        shape=[self.n_users, self.k],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
        "\n",
        "    # Creamos la matriz Q con los vectores latentes de los items\n",
        "    self.Q = self.add_variable('Q_matrix',\n",
        "        shape=[self.n_items, self.k],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Separamos los indice de usuarios e items\n",
        "    user_input = inputs[:, 0]\n",
        "    item_input = inputs[:, 1]\n",
        "\n",
        "    # Buscamos el vector latente de usuario e item con la\n",
        "    # función `embedding_lookup`\n",
        "    p = tf.nn.embedding_lookup(self.P, user_input, name='user_embedding_lookup')\n",
        "    q = tf.nn.embedding_lookup(self.Q, item_input, name='item_embedding_lookup')\n",
        "\n",
        "    # Multiplicamos los embedding de usuario e items\n",
        "    y_hat = tf.reduce_sum(tf.multiply(p, q), 1)\n",
        "\n",
        "    return y_hat, p, q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ryx145xXvR",
        "colab_type": "text"
      },
      "source": [
        "Ahora continuaremos implementando la capa externa del modelo heredando de la clase `tf.keras.Model`. Esta clase implementa los siguientes métodos:\n",
        "\n",
        "*   `__init__`: método que inicializa el modelo.\n",
        "*   `call`: implementa las operaciones del modelo y las capas internas.\n",
        "\n",
        "Recordemos que además de los métodos implementados, al heredar de la clase `Model` también tenemos los métdos `fit()`, `evaluate()`, `predict()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqySTfAKxX9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatrixFactorization(tf.keras.Model):\n",
        "  def __init__(self, n_users, n_items, k=32, alpha=.01):\n",
        "    super().__init__()\n",
        "    self.alpha = alpha\n",
        "    self.layer = FactorizationLayer(n_users, n_items, k=k)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Las predicciones del modelo\n",
        "    y_hat, p, q = self.layer(inputs)\n",
        "\n",
        "    # Además calculamos el regularizador\n",
        "    l2_loss = self.alpha * (tf.nn.l2_loss(p) + tf.nn.l2_loss(q))\n",
        "    self.add_loss(l2_loss)\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfuz10aezLhp",
        "colab_type": "text"
      },
      "source": [
        "Ahora instanciaremos y entrenaremos el modelo. Utilizando los métodos de la clase `Model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEOhSLihzOC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_users = data['userId'].nunique()\n",
        "n_items = data['movieId'].nunique()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "model = MatrixFactorization(n_users, n_items)\n",
        "model.compile(optimizer=optimizer, loss=mse_loss)\n",
        "model.fit(train_ds, \n",
        "          epochs=20, \n",
        "          verbose=1,\n",
        "          validation_data=val_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nprDgkn-5v3h",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx6BsWLvqJQA",
        "colab_type": "text"
      },
      "source": [
        "**Tensorboard** nos permite visualizar algunas variables del modelo y el grafo de cómputo generado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR-dvm4z5yEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime, os\n",
        "\n",
        "logdir = os.path.join('logs', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "model = MatrixFactorization(n_users, n_items)\n",
        "model.compile(optimizer=optimizer, loss=mse_loss)\n",
        "\n",
        "model.fit(train_ds, \n",
        "          epochs=10, \n",
        "          verbose=1,\n",
        "          validation_data=val_ds,\n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "tf.summary.trace_on(graph=True, profiler=True)\n",
        "with writer.as_default():\n",
        "  tf.summary.trace_export(\n",
        "      name='Tutorial_Recsys',\n",
        "      step=0,\n",
        "      profiler_outdir=logdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8RkdXB5WJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cargamos la extensión de tensorboard y lo ejecutamos\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}